<!DOCTYPE html>

<html>
    <head>
        <title>Projects</title>
        <!-- link to main stylesheet -->
        <link href="https://fonts.googleapis.com/css?family=Lato:300,400" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/css/main_1.css">
    </head>
    <body>

        <nav>
    		<ul>
        		<li><a href="/">Home</a></li>
	        	<!-- <li><a href="/about">About</a></li> -->
        		<li><a href="/assets/Pratyush_WPI_Resume.pdf">R‌&#233;sum‌&#233;</a></li>
        		<li><a href="/projects.html">Experience</a></li>
    		</ul>
		</nav>

        <h4> Research </h4>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          
          <tr>
              <td width="25%">
                <div class="one">
                <!-- <div class="two"><img src='friendly_after.png'></div> -->
                <img src='/assets/Projects/wheelchair.mp4' style="width:240px;height:140px;">
                </div>
              </td>
              <td valign="top" width="75%">
                  <p>Semi-Autonomous Stairclimbing Wheelchair<br></p>
                  <a href="http://krrish94.github.io/">Yogita Choudhary</a>,<a href="https://sites.google.com/view/nidhi-malhotra/home">Nidhi Malhotra<a>, Pratyush Kumar Sahoo<br> 
                <i>Indian Institute Of Technology, Varanasi</i><p2>   [Accepted for presentation at Student Design Competition, AIM Boston, 2020]</p2><br><br>          
                <p2>We  develop  a  low  cost  semi-autonomous,  robustmechanism  for  a  stair-climbing  wheelchair  in  this  work.  Thecore contribution is the system include the design of a variablegeometry track-based mechanism which accomplishes the taskof stair climbing. Additionally, for safety of the user we proposea mechanism to adjust the elevation of the seat through closed-loop   feedback   control.   For   attitude   estimation,   we   employthe  Extended  Kalman  Filter  Approach.  The  algorithm  fusesdata   from   IMU,   RGB   and   Depth   channels   of   the   Kinectsensor. Presence of both RGB and Depth data helps to receivean  accurate  pose  estimate  is  different  lighting  conditions.Thedynamics  of  the  robot  have  been  approximated  as  a  first-order linear system. A Proportional Integral (PI) Controller isimplemented  for  Heading  angle  control  to  maintain  a  desiredpath  of  the  wheelchair.  Slip  Compensation  is  done  using  theSlip  Compensated  Optometry  using  Gyro  approach.Finally,we  stress  test  our  algorithms  in  the  Robot  Operating  System(ROS) simulation software and prove that the algorithm worksefficiently in maintaining zero heading angle throughout ascend <a href="https://sites.google.com/itbhu.ac.in/sascw/home">[Project Page]</a> <a href="https://drive.google.com/file/d/1N-o31Qdj8-QFg7yBdQzoe6XPprs3yurg/view?usp=sharing">[Thesis]</a> <a href="https://ras.papercept.net/proceedings/AIM20/0505.pdf">[Poster]</a><br></p2>
              </td>
          </tr>
          <tr>

          <tr>
              <td width="25%">
                <div class="one">
                <!-- <div class="two"><img src='friendly_after.png'></div> -->
                <img src='/assets/Research/calibnet.gif' style="width:240px;height:140px;">
                </div>
              </td>
              <td valign="top" width="75%">
                  <p>CalibNet: Self-Supervised Extrinsic Calibration using 3D Spatial Transformer Networks<br></p>
                  Ganesh Iyer, <a href="http://karnikram.info">Karnik Ram</a>, <a href="http://krrish94.github.io/">Krishna Murthy</a>, <a href="http://robotics.iiit.ac.in/">K. Madhava Krishna</a><br>
                <i>Robotics Research Center, IIIT-H</i><p2>   [Accepted to IROS 2018]</p2><br><br>
                <anchor id="p_swaayatt_stereo" href=''></anchor>
                <p2>CalibNet is a self-supervised deep network capable of automatically estimating the 6-DoF rigid body transformation for extrinsic calibration between a 3D LiDAR and a 2D camera in real-time, by maximizing the photometric and geometric consistency between the input images and point clouds. <a href="https://epiception.github.io/CalibNet/">[Project Page]</a> <a href="https://arxiv.org/abs/1803.08181">[Paper]</a> <br></p2>
              </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
              <!-- <div class="two"><img src='friendly_after.png'></div> -->
              <img src='/assets/Research/ctc.png' style="width:240px;height:150px;">
              </div>
            </td>
            <td valign="top" width="75%">
                <p>Geometric Consistency for Self-Supervised End-to-End Visual Odometry<br></p>
                Ganesh Iyer*, <a href="http://krrish94.github.io/">Krishna Murthy*</a>, <a href="https://gunshi.github.io/">Gunshi Gupta</a>, <a href="http://robotics.iiit.ac.in/">K. Madhava Krishna, </a><br> Liam Paull
                <anchor id="p_research_calib" href=''></anchor>
              <br>
              <i>Robotics Research Center, IIIT-H and Montreal Institute for Learning Algorithms, Université de Montréal</i><p2>   [Accepted to CVPR-Workshop 2018]</p2><br><br>
              <p2>We propose an unsupervised paradigm for deep visual odometry learning. We show that using a noisy teacher, which could be a standard VO pipeline, and by designing a loss term that enforces geometric consistency of the trajectory, we can train accurate deep models for VO that do not require ground-truth labels. We leverage geometry as a self-supervisory signal and propose "Composite Transformation Constraints (CTCs)", that automatically generate supervisory signals for training and enforce geometric consistency in the VO estimate
              <a href="https://krrish94.github.io/CTCNet-release/">[Project Page]</a> <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w9/Iyer_Geometric_Consistency_for_CVPR_2018_paper.pdf">[Paper]</a> </p2>
            </td>
          </tr>
      </table>

      <h4> Projects </h4>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        
        <tr>
            <td width="25%">
              <div class="one">
                  <iframe width="240" height="150" src="https://www.youtube.com/embed/jfqZsi48lsk" frameborder="0" gesture="media"  allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
              </div>
            </td>
            <td valign="top" width="75%">
                <anchor id="p_main" href=''></anchor>
              <p> Team R.A.M.S: Robust Aerial Manipulation System<br></p>
              <i>Capstone Project | MBZIRC Challenge 2020,  Carnegie Mellon University</i><br>
              <p2>Participated in the design and development of an aerial manipulation platform capable of recognizing objects and lifting targeted payloads
                  upto 1.5kg using an onboard perception subsystem and visual servoing.<br>
              Check out our work! : <a href="https://mrsdprojects.ri.cmu.edu/2018teamc/">[Project Page]</a> | <a href="https://mrsdprojects.ri.cmu.edu/2018teamc/media/">[Demos]</a><br></p2>
            </td>
          </tr>

        <tr>
            <!-- <td width="25%">
              <div class="one">
                  <iframe width="240" height="150" src="https://www.youtube.com/embed/NoQE2QCoc9o" frameborder="0" gesture="media"  allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
              </div>
            </td> -->
            <td width="25%">
              <div class="one">
              <!-- <div class="two"><img src='friendly_after.png'></div> -->
              <img src='/assets/Projects/independent_study.gif' style="width:240px;height:150px;">
              </div>
            </td>
            <td valign="top" width="75%">
              <anchor id="p_embedded2" href=''></anchor>
              <p>Independent Study: Researching Skill Models for Autonomous Food Preparation<br></p> 
              <i>Carnegie Mellon University, advised by Prof. Oliver Kroemer</i><br>
              <p2>We address the task of building a large-scale data collection pipeline for interacting with food objects of different properties (like vegetables, steak, and dough) in order to extend learning-based methods to food preparation skills. Tested on hardware (FRANKA EMIKA PANDA), and in simulation (NVIDIA FLeX simulation package), the framework enables the data-driven learning of transition models of various food objects to infer properties from a few interactions and to plan for long-horizon food preparation tasks.<br>
              Report: <a href="/assets/Projects/Independent_Study.pdf">[Link]</a> | Open Source Code: <a>Coming soon</a></p2>
              <!-- <p2>Developing an unsupervised learning based framework that predicts the effects of simple manipulation skills on deformable objects like vegetables and dough. Currently being tested on the FRANKA EMIKA Panda arm.<br> -->
            </td>
             
          </tr>

        <tr>
          <td width="25%">
            <div class="one">
            <!-- <div class="two"><img src='friendly_after.png'></div> -->
            <img src='/assets/Projects/16881_project.gif' style="width:200px;height:180px;">
            </div>
          </td>
          <td valign="top" width="75%">
            <anchor id="p_embedded2" href=''></anchor>
            <p>Learning Diverse Goal-Conditioned Policies for Frontier Selection in Navigation<br></p> 
            <i>Course Project: Special Topics - Deep Reinforcement Learning for Robotics, Carnegie Mellon University</i><br>
            <p2>In this work, we propose a hierarchical formulation to tackle the problem of learning based efficient exploration. We decompose the the task into two sub-problems: selecting the next best goal in the visible space, followed by efficiently navigating to this sub-goal in the partial map setting. We propose a global policy network that learns the selection of the next best goal(‘frontier’) in the observable space, followed by a local policy that learns low-level navigation conditioned on different goal embeddings in known environments<br>
            Report: <a href="/assets/Projects/16881_project.pdf">[Link]</a> | Open Source Code: <a>Coming soon</a></p2>
            <!-- <p2>Developing an unsupervised learning based framework that predicts the effects of simple manipulation skills on deformable objects like vegetables and dough. Currently being tested on the FRANKA EMIKA Panda arm.<br> -->
          </td>
            
        </tr>

        <tr>
          <td width="25%">
            <div class="one">
            <!-- <div class="two"><img src='friendly_after.png'></div> -->
            <img src='/assets/Projects/eventvo.png' style="width:240px;height:120px;">
            </div>
          </td>
          <td valign="top" width="75%">
            <p>DeepEvent-VO: Fusing Intensity Images and Event Streams for End-to-End Visual Odometry<br></p>
            <i>Course Project: Robot Localization and Mapping, Carnegie Mellon University</i><br>
            <!-- <a href="https://sampa.cs.washington.edu/projects/vr-hw.html">project page</a> -->
            <p2>Designed a recurrent convolutional network that fuses intensity and event based image feature streams to make continuous visual odometry predictions for high speed applications using event-based cameras.<br>
            Open Source Code: <a href="https://github.com/SuhitK/DeepEvent-VO">[Github Repository]</a> | Report: <a href="/assets/Projects/deep_event_vo.pdf">[Link]</a><br></p2>
          </td>
        </tr>

        <tr>
          <td width="25%">
            <div class="one">
            <!-- <div class="two"><img src='friendly_after.png'></div> -->
            <img src='/assets/Projects/montage_horz_low.png' style="width:240px;height:160px;">
            </div>
          </td>
          <td valign="top" width="75%">
            <anchor id="p_swaayatt_facepose" href=''></anchor>
            <p>Stereo Pipeline: Deep Convolutional Network, Semi-Global Matching and Pointcloud Reconstruction<br></p>
            <i>Swaayatt Robots, India</i><br>
            <!-- <a href="https://sampa.cs.washington.edu/projects/vr-hw.html">project page</a> -->
            <p2>Implementation and Improvement of pipeline in <a href = "http://jmlr.csail.mit.edu/papers/volume17/15-535/15-535.pdf">Zbontar et. al.</a> for fast Disparity Map Computation and point cloud reconstruction of output Depth Map.<br>
            Open Source Code: <a href="https://github.com/epiception/theano-mc-cnn">[Deep Network]</a> | <a href="https://github.com/epiception/SGM-Census">[Census Transform and Semi-Global Matching]</a><br></p2>
          </td>
        </tr>

        <tr>
          <td width="25%">
            <div class="one">
            <img src='/assets/Projects/face_clouds_low.png' style="width:240px;height:140px;">
            </div>
          </td>
          <td valign="top" width="75%">
            <anchor id="p_swaayatt_tracker" href=''></anchor>
            <p>RGBD Facial Pose tracking for Advanced Driver Assistance Systems<br></p>
            <i>Swaayatt Robots, India</i><br>
            <p2>Point Cloud Processing package for tracking the face pose and central axis of gaze for RGBD based Advanced Driver
                Assistance System. Alignment to standard model using 3D FPFH features for points and normals, Sampling Consensus and Iterative Closest Point.</p2>

          </td>
        </tr>

        <tr>
          <td width="25%">
            <div class="one">
            <img src='/assets/Projects/tracker_segmenter.jpg' style="width:240px;height:140px;">
            </div>
          </td>
          <td valign="top" width="75%">
            <anchor id="p_embedded1" href=''></anchor>
            <p>Tracking and Segmentation of Vehicles for Annotation<br></p>
            <i>Swaayatt Robots, India</i><br>
            <p2>Annotation system for vehicle detection data. Propogating selected keypoints and feature points within vehicle boundaries using multi-scale template matching and particle filters. Results in a scale-changing contour of vehicles to be segmented. </p2>
          </td>
        </tr>

        <tr>
          <td width="25%">
            <div class="one">
                <iframe width="240" height="180" src="https://www.youtube.com/embed/YWjOEWO5lFs" frameborder="0" gesture="media" allowfullscreen></iframe>
            </div>
          </td>
          <td valign="top" width="75%">
            <anchor id="p_embedded2" href=''></anchor>
            <p>Telepresence Robot with Stereoscopic Vision<br></p>
            <i>Final Year Project, TCET, Mumbai University</i><br>
            <p2>Small scale and inexpensive telepresence platform capable of streaming immersive 3D SBS live video feed (350x350 resolution, 40 fps). Base platform actuated using AtMega2560 and keyboard/console commands. Raspberry Pi with camera module receives axis-angle commands from smartphone to control a 2-DOF servo-gimbal.<br>
            Code: <a href="https://github.com/epiception/Virtual-Telepresence">[Telepresence bot]</a><br></p2>
          </td>
        </tr>

        <tr>
          <td width="25%">
            <div class="one">
                <iframe width="240" height="120" src="https://www.youtube.com/embed/yTIguJIK16Y" frameborder="0" gesture="media" allowfullscreen></iframe><br>
                <iframe width="240" height="120" src="https://www.youtube.com/embed/YmRXB2YnW48" frameborder="0" gesture="media" allowfullscreen></iframe>
            </div>
          </td>
          <td valign="top" width="75%">
            <p>Grid Traversing Robots<br></p>
            <i>eYantra Lab Setup Initiative, TCET, Mumbai University</i><br>
            <p2>1. Minesweeping Robot: Traverses a small grid to locate basic obstacles (mines) and display their co-ordinate locations after reaching the end point. Breadth First Search and Djikstras' Algorithms based traversal are demonstrated.<br><br>
            2. Warehouse Management Simulation: Implementation of a small scale automated supply chain using order picking algorithms. Objects are collected, sorted based on a requirement (eg. color) and then transported to the specified destination zone.<br><br>
            Developed on the Firebird-V AtMega2560 Robotics Research Platform, Nex Robotics, IIT-Bombay.
            Code: <a href="https://github.com/epiception/Mine-Localization-Robot-using-Firebird-V">[Mine Localization]</a> | <a href="https://github.com/epiception/Warehouse-Management-Using-FireBird-V">[Warehouse Management]</a><br></p2>
          </td>
        </tr>
      </table>

      <footer>
          <a style="float: right; padding-top: 25px;" href="https://jonbarron.info/">Template Credits</a>
          <!-- <ul>
              <li><a href="mailto:giyer2309@gmail.com">email</a></li>
              <li><a href="https://github.com/epiception">github.com/epiception</a></li>
          </ul> -->
          <div class="footer-social-icons">
              <!-- <h4 class="_14">Follow us on</h4> -->
              <ul class="social-icons">
                  <li><a href="mailto:giyer2309@gmail.com" class="social-icon"> <i class="fa fa-envelope"></i></a></li>
                  <li><a href="https://www.linkedin.com/in/ganesh-iyer-0607bb112/" class="social-icon"> <i class="fa fa-linkedin"></i></a></li>
                  <li><a href="https://github.com/epiception" class="social-icon"> <i class="fa fa-github"></i></a></li>
                  <li><a href="https://www.youtube.com/user/giyer2309/videos?view_as=subscriber&sort=dd&view=0&shelf_id=0" class="social-icon"> <i class="fa fa-youtube"></i></a></li>
              </ul>
          </div>
      </footer>
    </body>

</html>
